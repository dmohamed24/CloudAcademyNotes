### What is Caching

- Caching is the process of storing copies of files in a cache, or temporary storage location, so that they can be accessed more quickly.
- Caching improvers the performance and latency of the application as retrieving data from the original data source (e.g., a database or external API) requires higher computational power making the data retrieval slower
- In a cache data is stored in a hierarchal manner, so when a data request is made and it hits the cache it will look in L1 if the data is not found there then it looks in L2 so on and so fourth till the data is found. 

### Chace hit 
- When the requested data is successfully found retrieved and read from the cache is described as a cache hit 
- A cache hit can be classified in to 3 groups based on the speed of the data retrieval
	1. Cache hot: This is when the data is read at the fasted possible rate and this happens when the data exists in L1
	2. Cache warm: This describes scenarios were the requested data is read from L2 or L3 
	3. Cache cold: This is when the data is read at the slowest possible rate and this happens when the data exists in L3 or lower 

### Chace miss
When the requested data is not found in the cache which the requires it to be read from the original data source and written in to the cache for late retrieval of that data 


### Cache invalidation 

Cache invalidation is the process of updating or removing a data stored in cache when it is no longer valid or useful. There are a few types of caching patterns used for cache invalidation 

#### Cache aside Patten 
- This is were the cache only talks to the server and never the database. 
- When there is a cache miss the server will retrieve the data form the database and write into the cache for access more efficiently for the next time. 
- Similarly if the database values have been updated then the server will be responmsible in updating the cache as well  
- In a cache hit scenario the server will retrieve the requested data from the cache and not need to talk to the database
##### Advantages 
- If the cache fails or it goes down the application will still be able to serve any requested but the speed at retrieving the data will be affected 
#### Disadvantages 
- Server is required to constantly update the cache when the database changes 

#### Read through pattern 
-  The cache sits between the application (client and server) and the database. So the application will always talk to the cache and never the database directly 
-  The cache is responsible for writing to the database and updating the database when the database changes 
-  The data modelling for the cache and database has to be the same and the cache itself is from a third party provider 
##### Advantages 
- Combines read and write operations.
-  The database and cache are updated simultaneously and will be in sync 
- This pattern is very effective for read heavy application like news feed as the data can be retrieved very quickly from the cache instead from the database 
#### Disadvantages 
- On a cache miss, data is fetched from the original source, stored in the cache, and then returned to the requester.
- If the cache goes down the application will not be able to serve any content since the cache is responsible to communicating with the database. Multiple caches can be used to mitigate this issue but it will be expensive since all the caches will have to be updated simultaneously and contain the same data

#### Write through pattern 
- The cache sits between the application (client and server) and the database. So the application will always talk to the cache and never the database directly 
- The cache is responsible for writing to the database and updating the database when the database changes 
- The data modelling for the cache and database has to be the same  and the cache itself is from a third party provider 
##### Advantages 
- Combines read and write operations.
- The database and cache are updated simultaneously and will be in sync 
#### Disadvantages 
- For write operation there is an extra step redundant step in the process which is that the the cache and database are updated simultaneously which takes longer. This set up is more useful when more read heavy systems
- If the cache goes down the application will not be able to serve any content since the cache is responsible to communicating with the database. Multiple caches can be used to mitigate this issue but it will be expensive since all the caches will have to be updated simultaneously and contain the same data

#### Write around pattern 
- In this set up the application talks to both the cache and database but for different operations. 
- For read operations the app will retrieve the data from the cache 
- For write operations the app will send the data to the database directly without simultaneously writing in the the cache for an update 
##### Advantages 
- This set up is more effective performance wise for write heavy application since the app updates the database directly
#### Disadvantages 
- In this scenario the cache and database will not in sync when data is requested since the cache was not updated during the write operations to there will be more cache miss scenarios 

#### Write around pattern 
- In this set up the application talks to both the cache and database but for different operations. 
- For read operations the app will retrieve the data from the cache 
- For write operations the app will send the data to the database directly without simultaneously writing in the the cache for an update 
##### Advantages 
- This set up is more effective performance wise for write heavy application since the app updates the database directly
#### Disadvantages 
- In this scenario the cache and database will not in sync when data is requested since the cache was not updated during the write operations to there will be more cache miss scenarios 
### Write back pattern 
- Similarly in the  read/write through set up the applications only communicates with the cache and the cache is responsible for updating the database in write operations 
- The difference is that the cache updates the database asynchronously and in batches. So the cache will store a number of write operations and after a certain condition is met e.g. (after a certain amount of time, number of batches reached etc ) the cache will update to the database with all the write operations in one go
##### Advantages 
- This set up is more effective performance wise for write heavy application since the app updates the database directly
- This set up can also handle a scenario were the database is down and the cache assume the responsibility of the database for that time
#### Disadvantages 
-  If the cache goes down the application will not be able to serve any content since the cache is responsible to communicating with the database. 
- Also with the cache down it will lose all the batched write operation it stored 
- To mitigate this multiple caches can be used 

### Cache pattern summary 

![[Screenshot 2024-07-13 172459.png]]

### Cache eviction
Following are some of the most common cache eviction policies:

- **First In First Out (FIFO)**: The cache evicts the first block accessed first without any regard to how often or how many times it was accessed before.
- **Last In First Out (LIFO)**: The cache evicts the block accessed most recently first without any regard to how often or how many times it was accessed before.
- **Least Recently Used (LRU)**: Discards the least recently used items first.
- **Most Recently Used (MRU)**: Discards, in contrast to LRU, the most recently used items first.
- **Least Frequently Used (LFU)**: Counts how often an item is needed. Those that are used least often are discarded first.
- **Random Replacement (RR)**: Randomly selects a candidate item and discards it to make space when necessary.

### Types of caching 
Caching is a versatile strategy used to improve the performance, scalability, and efficiency of systems. Different types of caching are employed based on the use case and specific requirements:

1. **Client-Side Caching:** Improves client performance by storing resources locally.
2. **Server-Side Caching:** Reduces server load and speeds up response times.
3. **Proxy Caching:** Serves content from intermediary servers, improving performance and reducing bandwidth.
4. **CDN Caching:** Delivers content from geographically distributed edge servers to reduce latency.
5. **Database Caching:** Speeds up database access by caching queries or data in memory.
6. **Application-Level Caching:** Enhances application performance by caching data and computations.
7. **Distributed Caching:** Provides scalable and fault-tolerant caching across multiple nodes.
8. **Cache Hierarchies:** Utilizes multiple caching layers for optimized performance.
9. **Write Strategies:** Ensures data consistency and performance based on write patterns and requirements.

### Distributed Caching

**Definition:** Distributed caching is a caching mechanism where the cache is spread across multiple nodes or servers, typically within a single data center or across multiple data centers. This approach ensures scalability, high availability, and fault tolerance by distributing the cache storage and access load across several machines.
### Global Caching

**Definition:** Global caching, also known as geo-distributed caching, is a caching mechanism that distributes cache data across multiple geographic locations around the world. This approach ensures that users from different regions can access cached data from the nearest location, reducing latency and improving performance.

### Comparison: Distributed Caching vs. Global Caching

| Aspect               | Distributed Caching                                                      | Global Caching                                           |
| -------------------- | ------------------------------------------------------------------------ | -------------------------------------------------------- |
| Geographic Scope     | Within a single data center or region                                    | Across multiple geographic locations globally            |
| Primary Goal         | Scalability, high availability, and fault tolerance within a data center | Reduced latency and improved performance globally        |
| Data Distribution    | Partitioned and replicated within nodes in a cluster                     | Geo-replicated across nodes in different regions         |
| Example Technologies | Redis Cluster, Apache Ignite, Amazon ElastiCache                         | CDNs like Cloudflare, Akamai, Amazon CloudFront          |
| Use Cases            | High-traffic applications, microservices architectures                   | Global websites, streaming services, global applications |

- **Distributed Caching:** Focuses on scalability, high availability, and performance within a single data center or region by distributing cache data across multiple nodes.
- **Global Caching:** Extends the caching mechanism to a global scale, ensuring reduced latency and high performance for users worldwide by distributing cache data across multiple geographic locations.

### When not to use caching 

- Caching isn't helpful when it takes just as long to access the cache as it does to access the primary data store.
- Caching doesn't work as well when requests have low repetition (higher randomness), because caching performance comes from repeated memory access patterns.
- Caching isn't helpful when the data changes frequently, as the cached version gets out of sync, and the primary data store must be accessed every time.

### Overall Advantages of caching 

Below are some advantages of caching:

- Improves performance
- Reduce latency
- Reduce load on the database
- Reduce network cost
- Increase Read Throughput