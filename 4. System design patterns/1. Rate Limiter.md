### What is a rate limiter 
A rate limiter is a technique that controls the rate at which requests are made to a network, server, or other resource. It can be used to limit the number of requests that can be made within a certain time frame. Rate limiting can help to: 

- Prevent malicious bot activity 
- Reduce strain on web servers and cost
- Protect against denial-of-service (DoS) attacks 
- Ensure fair use of services 
- Prevent resource abuse 
- Limit web scraping 

Rate limiting works by measuring the time between requests from an **IP address** or **User_id** and tracking the number of requests made within a specific time frame. If an IP address makes too many requests within that time frame, the rate limiter will throttle the IP address and not fulfill its requests for the next time frame. Rate-limited applications can also notify individual users to slow down if they make requests too frequently.

### Rate limiter requirements 

1. 1 billion users 
2. Low latency
3. Distributed rate limiting: can be used and shared across multiple servers
4. Handle Exceptions: Show response to users when their request is throttled
5. Should have high fault tolerance: if a component of the rate limiter is down the should not affect the entire system 
6. user_id  = 8bytes 
7. Tacking number of requests made to an endpoint = 4bytes 
8. Storage calculation = 1billion * 20 * (8+4) = 240gb (Would require data partitioning)

### What to rate limit ?
 - **User_Id**:
	 - Easy to track as a single user can be given a **user_id**, so for authenticated endpoints we can pass in the **User_Id** to track the number of requests being made by that user and throttled the requests if rate limit is exceeded
	 - Downside to this is that bad actors can create multiple accounts which means that the **User_Id** will change so its harder to keep track in these scenarios just using **User_Id**
	 - Also for non authenticated endpoints (logged out state) no **User_Id** is provided at this point 
- **Ip address**: 
	 - Now users in a logged out state can be tracked using the ip address as that remains constant unless its rotated  
	 - However good actors can be throttled who are on the same ip address
-  **Hybrid Solution**: 
	 - For non authenticated endpoints use the IP ADDRESS for rate limiting 
	 - For non authenticated endpoints use the USER_ID for rate limiting 


### PSUEDO CODE

```
rateLimit(userID,ipAddress,endpoint, date, requstTime )

UserID ==> long & nullable
endpoint ===> String
date ===> String
requestTime ===> String
```

### Where does the rate limiter live ?

##### Client-side rate limiters

These are easy to implement, but can be less effective than server-side rate limiters. A malicious actor could change the client-side code to bypass the rate limiter. Client-side rate limiters can help reduce a device's network usage and bandwidth consumption by preventing data from being sent

##### Server-side rate limiters

These are a common method for rate limiting, and are in the service provider's interest to ensure performance and availability. Server-side processes have access to the server's resources, such as its CPU, memory, storage, and any databases or other servers that the web application uses. 

### Server side rate limiter 

Rate limiters in the server side are usually placed as middleware, so requests made to a certain endpoint will go though that rate limiter middleware which will make the decision to to allow the request to go through or throttle and reject the request and returns a status code of `429` to the client which means the user has made too many requests

For example if an API allows 5 request per minute the 6th request in that minute will be throttled


![[Pasted image 20240820151807.png]]

![[Pasted image 20240820151838.png]]
#### API gateway

An API gateway is an application programming interface (API) management tool that serves as a single point of entry into a system, sitting between the application user and a collection of backend services

API gateways can handle common tasks such as: 

- Security policy: Authentication, authorization, access control, and encryption 
- Routing policy: Routing, rate limiting, request/response manipulation, circuit breaker, blue-green and canary deployments, A/B testing, load balancing, health checks, and custom error handling 
- Other tasks: Statistics, billing, monitoring, analytics, policies, alerts, and input validation

### Rate limiter algorithms 

- Token bucket
- Leaking bucket
- Fixed window counter
- Sliding window counter

### Rate limiter architecture 

The basic idea of a rate limiter is that at a high level we need a counter to keep track of how many requests are sent by the same user, ip address etc. If the counter is larger than the limit in a specific time frame the request is denied 

The counter needs to be stored somewhere and using a database is not a good idea with how slow data retrieval is from a disk. In memory cache is a good idea beacuse it is fast ands supports time base expiration. Redis.

##### Redis 
- Redis  is an open-source, in-memory, key/value store and NoSQL database that's often used as a cache or quick-response database. 
- It's known for being fast and durable because it stores and serves data from memory instead of a disk or solid-state drive (SSD). RAM is 150,000 times faster than accessing a disk, and 500 times faster than accessing SSD.
- Redis has 2 commands 
	-  INCR: it increases the stored counter by 1 
	- EXPIRED: It sets a timeout for the counter. If the timeout expires, the counter us automatically deleted 

#### RL design with redis

![[Pasted image 20240820155700.png]]

1. The clinet sends a request to the rate limiter 
2. The rate limiter fetches the counter of this specific user from the in memory key/value store db in redis and checks if the limit is reached or not
	- If the limit is reached the the request is rejected and throttled
	- If the limit is not reached, the request is forwarded to the api server. And the system increments the counter by 1 and saves back to redis


#### RL rules 

#### Rate limiting rule config example 

```
domain: auth
descriptor: 
	- key: auth_type
	- value: login
	- rate_limit:
		- unit: minute
		- request_per_minute: 5
```

#### Handling rate limits 
There are 2 options when it comes to handling requests that are throttled, we can either just drop the request entirely or we may en-queue the rate-limited request to be proceed later. In both scenarios the client will be send a response of status code `429`. 

##### Handling message ques 

1. **Request Received:** A client sends 20 requests per second, but the rate limit is set to 10 requests per second.
2. **Rate Limiting Check:**
    - The first 10 requests are processed immediately (non-throttled).
    - The remaining 10 requests exceed the rate limit and are throttled, then enqueued in the message queue.
3. **Queue Processing:**
    - Workers dequeue 5 requests per second from the message queue and process them.
    - The system continues to accept and process new non-throttled requests while processing throttled requests from the queue.
4. **Throttling Adjustments:** If the queue length increases, indicating a processing delay, the system could dynamically increase the number of workers to process the queued requests faster.

#### Rate limiter HHTP response headers

- `X-Ratelimit-Remaining`: The remaining number of allowed requests within a time frame 
- `X-Ratelimit-Limit`: It indicates how many calls the client can make per time frame
- `X-Ratelimit-Retry-After` : The number of seconds to wait until; you can make a request again without being throttled. This header is returned to client as a response when too many request are made 
#### Detailed design 

![[Pasted image 20240820165312.png]]

- Rules are stored in a disk/hard drive/ ssd. Workers will pull the rules from the storage and store them in a cache for quicker and easier access for rate limiter middleware 
- When the client sends a request to the server , the request goes through the rate limiter middleware
- Rate limiter pulls the rate limiting rules for a specific service from the cache. It then fetches the counter and request time stamps from the redis cache for that specific user and decides accordingly to throttle the request or not 
- If the request is throttled it can either be dropped or sent to a message queue to be proceed separately if there is for example a system overload but for both scenarios the client is sent a `429` status code

*Workers*: these are background processors 

### Distributed Rate limiter design 

Scaling a rate limiter for a distributed system to support multiple servers can be challenging
	 - Race condition
	 - Synchronization issue

#### Race condition

In this scenario 2 requests concurrently read the value of a counter from redis before either request is able to increment on the counter value by 1. Each server can will read the same value and write back to the redis db the same value without checking any other operations done by another thread.  

To solve this problem you8 can enforce locking but that will significantly slow down the system if implemented. The 2 strategies are commonly used are **lua scripts** and **sorted sets data structure in  redis**

#### Synchronization issue 

Synchronization is an important factor top consider when it comes to distributed environments. To support millions of users 1 rate limiter will not be enough to handle that sort of traffic. When multiple rate limiter servers are used synchronization of the data is needed. In a state less environment client request can routed to different rate limiters so the data needs to be synced up so any server can assess the latest and same data.

- A solution can be to use sticky session to allow clients o send requetss to the same rate limiter each time. Big problem with this solution is that its not scalable and flexible at all .
- Best solution is to use a centralized redis db that servers can access the latest and same data as each server. 

